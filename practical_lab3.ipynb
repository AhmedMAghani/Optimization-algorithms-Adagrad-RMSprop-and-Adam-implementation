{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Generating the data\n",
    "<b>yi = a*xi + b (and a = -2, b = 1)</b>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.linspace(0,20,50)\n",
    "y = -2*x+1\n",
    "x = x[:,np.newaxis]\n",
    "y = y[:,np.newaxis]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch/Vanilla, Mini-Batch and stochastic Gradient Descent implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initialize_weights(col_dim,row_dim=1):\n",
    "    return np.random.normal(size=(row_dim,col_dim))*0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initialize_bias(weights_array):\n",
    "    return np.insert(weights_array,0,0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initialize_theta(features_number):\n",
    "    weights = initialize_weights(features_number)\n",
    "    return initialize_bias(weights).reshape([-1,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(input_data,weights):\n",
    "    return np.matmul(input_data,weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_mean_square_error(sample_number,output,predicted):\n",
    "    return (1/(2*sample_number))*np.sum(np.power((predicted-output),2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_gradient(number_of_samples,predicted,output,input):\n",
    "    return (1/number_of_samples)*np.sum((predicted-output)*input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_new_weights(weights,d_theta,lr):\n",
    "    return (weights - (lr*d_theta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_data(training_data,real_output,samples_number,batches):\n",
    "    samples_per_batch = samples_number//batches\n",
    "    data_batches = {}\n",
    "    batch_data ={}\n",
    "    current_batch_start_index = 0\n",
    "    current_batch_end_index = samples_per_batch\n",
    "    for batch in range(1,batches+1):\n",
    "        batch_data[\"training_batch_data\"] = training_data[current_batch_start_index:current_batch_end_index]\n",
    "        batch_data[\"real_output\"] = real_output[current_batch_start_index:current_batch_end_index]\n",
    "        data_batches[batch] = batch_data.copy()\n",
    "        current_batch_start_index += samples_per_batch\n",
    "        if batch == batches:\n",
    "            current_batch_end_index = len(training_data)\n",
    "        else:\n",
    "            current_batch_end_index += samples_per_batch\n",
    "    return data_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_theta_history(theta_matrix):\n",
    "    theta_dict = {}\n",
    "    for _,matrix in theta_matrix.items():\n",
    "        for index,theta in enumerate(matrix):\n",
    "            if \"theta_\"+str(index) in theta_dict.keys():\n",
    "                theta_dict[\"theta_\"+str(index)].append(theta)\n",
    "            else:\n",
    "                theta_dict[\"theta_\"+str(index)] = [theta]\n",
    "    return theta_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_using_gradient_descent(training_data,real_output,epochs,lr=0.0001,batches=1):\n",
    "    cost_history = {}\n",
    "    theta_history = {}\n",
    "    epoch_index = 0\n",
    "    samples_number = training_data.shape[0]\n",
    "    features_number = training_data.shape[1] - 1\n",
    "    theta_matrix = initialize_theta(features_number)\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for batch in range(1,batches+1):\n",
    "            data_batches = split_data(training_data,real_output,samples_number,batches)\n",
    "            training_data_batch = data_batches[batch][\"training_batch_data\"]\n",
    "            real_output_data_batch = data_batches[batch][\"real_output\"]\n",
    "            batch_samples_number = training_data_batch.shape[0]\n",
    "            predicted_output = predict(training_data_batch,theta_matrix)\n",
    "            cost = calc_mean_square_error(batch_samples_number,real_output_data_batch,predicted_output)\n",
    "            d_theta = calc_gradient(batch_samples_number,predicted_output,real_output_data_batch,training_data_batch)\n",
    "            theta_matrix = calc_new_weights(theta_matrix,d_theta,lr)\n",
    "            cost_history[batch+epoch_index] = cost\n",
    "            theta_history[batch+epoch_index] = theta_matrix\n",
    "            print(f\"Epoch:{epoch} of {epochs}, Batch: {batch}\\nCost: {cost}, Gradient: {d_theta}\")\n",
    "        print(f\"R2 Score: {r2_score(real_output,predict(training_data,theta_matrix))}\")\n",
    "        if epoch > 1 and (d_theta <= 0.00001 or sum(theta_history[batch+epoch_index-2]-theta_matrix)/len(theta_matrix) <= 0.00001 or cost_history[batch+epoch_index-2]-cost <= 0.00001):\n",
    "            break\n",
    "        epoch_index += (batches+1)\n",
    "    theta_history = prepare_theta_history(theta_history)\n",
    "    predicted_output = predict(training_data,theta_matrix)\n",
    "    return {\"weights_matrix\":theta_matrix,\"predicted_output\":predicted_output,\"epochs_history\":cost_history,\"theta_history\":theta_history}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_theta(fig,cost_history,theta_history):\n",
    "    index = 413\n",
    "    for key,value in theta_history.items():\n",
    "        ax = fig.add_subplot(index)\n",
    "        ax.xaxis.label.set_color('white')\n",
    "        ax.yaxis.label.set_color('white')\n",
    "        ax.tick_params(axis='x', colors='white')\n",
    "        ax.tick_params(axis='y', colors='white')\n",
    "        ax.plot(value,np.array(list(cost_history.values())).flatten(),\"-r\",label=\"predicted output\")\n",
    "        ax.set_xlabel(\"loss\")\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid()\n",
    "        ax.legend()\n",
    "        index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_input_data_for_training(data):\n",
    "    if len(data.shape)<2:\n",
    "        data = data[:,np.newaxis]\n",
    "    return np.insert(data,0,1,axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_graph(fig,raw_input_data,predicted_output,raw_output_data):\n",
    "    ax1 = fig.add_subplot(411)\n",
    "    ax1.xaxis.label.set_color('white')\n",
    "    ax1.yaxis.label.set_color('white')\n",
    "    ax1.tick_params(axis='x', colors='white')\n",
    "    ax1.tick_params(axis='y', colors='white')\n",
    "    ax1.plot(raw_input_data,raw_output_data,\"bo\",label=\"true output\")\n",
    "    ax1.plot(raw_input_data,predicted_output,\"-r\",label=\"predicted output\")\n",
    "    ax1.set_xlabel(\"input\")\n",
    "    ax1.set_ylabel(\"output\")\n",
    "    ax1.grid()\n",
    "    ax1.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_debug_graph(fig,history):\n",
    "    ax2 = fig.add_subplot(412)\n",
    "    ax2.xaxis.label.set_color('white')\n",
    "    ax2.yaxis.label.set_color('white')\n",
    "    ax2.tick_params(axis='x', colors='white')\n",
    "    ax2.tick_params(axis='y', colors='white')\n",
    "    ax2.plot(np.array(list(history.values())).flatten(),\"-bo\",label=\"true output\")\n",
    "    ax2.set_xlabel(\"Batches\")\n",
    "    ax2.set_ylabel(\"Cost\")\n",
    "    ax2.grid()\n",
    "    ax2.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    raw_input_data = x\n",
    "    raw_output_data = y\n",
    "    prepared_input_data = prepare_input_data_for_training(raw_input_data)\n",
    "    model_results = solve_using_gradient_descent(prepared_input_data,raw_output_data,100,0.001,1)\n",
    "    fig = plt.figure(figsize=(10, 30), dpi=100)\n",
    "    print_graph(fig,raw_input_data,model_results[\"predicted_output\"],raw_output_data)\n",
    "    print_debug_graph(fig,model_results[\"epochs_history\"])\n",
    "    print_theta(fig,model_results[\"epochs_history\"],model_results[\"theta_history\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Momentum Gradient Descent implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_new_momentum_weights(weights,d_theta,lr,gamma,momentum):\n",
    "    momentum = ((gamma*momentum)+(lr*d_theta))\n",
    "    return (weights - momentum),momentum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_using_momentum_gradient_descent(training_data,real_output,epochs,gamma,lr=0.0001,batches=1):\n",
    "    cost_history = {}\n",
    "    theta_history = {}\n",
    "    momentum = 0\n",
    "    epoch_index = 0\n",
    "    samples_number = training_data.shape[0]\n",
    "    features_number = training_data.shape[1] - 1\n",
    "    theta_matrix = initialize_theta(features_number)\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for batch in range(1,batches+1):\n",
    "            data_batches = split_data(training_data,real_output,samples_number,batches)\n",
    "            training_data_batch = data_batches[batch][\"training_batch_data\"]\n",
    "            real_output_data_batch = data_batches[batch][\"real_output\"]\n",
    "            batch_samples_number = training_data_batch.shape[0]\n",
    "            predicted_output = predict(training_data_batch,theta_matrix)\n",
    "            cost = calc_mean_square_error(batch_samples_number,real_output_data_batch,predicted_output)\n",
    "            d_theta = calc_gradient(batch_samples_number,predicted_output,real_output_data_batch,training_data_batch)\n",
    "            theta_matrix,momentum = calc_new_momentum_weights(theta_matrix,d_theta,lr,gamma,momentum)\n",
    "            cost_history[batch+epoch_index] = cost\n",
    "            theta_history[batch+epoch_index] = theta_matrix\n",
    "            print(f\"Epoch:{epoch} of {epochs}, Batch: {batch}\\nCost: {cost}, Gradient: {d_theta}\")\n",
    "        print(f\"R2 Score: {r2_score(real_output,predict(training_data,theta_matrix))}\")\n",
    "        if epoch > 1 and (d_theta <= 0.00001 or sum(theta_history[batch+epoch_index-2]-theta_matrix)/len(theta_matrix) <= 0.00001 or cost_history[batch+epoch_index-2]-cost <= 0.00001):\n",
    "            break\n",
    "        epoch_index += (batches+1)\n",
    "    theta_history = prepare_theta_history(theta_history)\n",
    "    return {\"weights_matrix\":theta_matrix,\"predicted_output\":predicted_output,\"epochs_history\":cost_history,\"theta_history\":theta_history}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    raw_input_data = x\n",
    "    raw_output_data = y\n",
    "    prepared_input_data = prepare_input_data_for_training(raw_input_data)\n",
    "    model_results = solve_using_momentum_gradient_descent(prepared_input_data,raw_output_data,500,0.9,0.001)\n",
    "    fig = plt.figure(figsize=(10, 30), dpi=100)\n",
    "    print_graph(fig,raw_input_data,model_results[\"predicted_output\"],raw_output_data)\n",
    "    print_debug_graph(fig,model_results[\"epochs_history\"])\n",
    "    print_theta(fig,model_results[\"epochs_history\"],model_results[\"theta_history\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nesterov Accelerated Gradient Descent implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_temp_weights(weights,gamma,momentum):\n",
    "    return (weights - (gamma*momentum))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_new_nesterov_weights(temp_weights,temp_d_theta,lr,gamma,momentum):\n",
    "    momentum = ((gamma*momentum)+(lr*temp_d_theta))\n",
    "    return (temp_weights - (lr*temp_d_theta)),momentum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nesterov_acceleration_weight_update(theta_matrix,gamma,lr,momentum,training_data_batch,batch_samples_number,real_output_data_batch):\n",
    "    temp_theta_matrix = calc_temp_weights(theta_matrix,gamma,momentum)\n",
    "    temp_predicted_output = predict(training_data_batch,temp_theta_matrix)\n",
    "    temp_d_theta = calc_gradient(batch_samples_number,temp_predicted_output,real_output_data_batch,training_data_batch)\n",
    "    return calc_new_nesterov_weights(temp_theta_matrix,temp_d_theta,lr,gamma,momentum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_using_nesterov_accelerated_gradient_descent(training_data,real_output,epochs,gamma,lr=0.0001,batches=1):\n",
    "    cost_history = {}\n",
    "    theta_history = {}\n",
    "    momentum = 0\n",
    "    epoch_index = 0\n",
    "    samples_number = training_data.shape[0]\n",
    "    features_number = training_data.shape[1] - 1\n",
    "    theta_matrix = initialize_theta(features_number)\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for batch in range(1,batches+1):\n",
    "            data_batches = split_data(training_data,real_output,samples_number,batches)\n",
    "            training_data_batch = data_batches[batch][\"training_batch_data\"]\n",
    "            real_output_data_batch = data_batches[batch][\"real_output\"]\n",
    "            batch_samples_number = training_data_batch.shape[0]\n",
    "            predicted_output = predict(training_data,theta_matrix)\n",
    "            cost = calc_mean_square_error(batch_samples_number,real_output_data_batch,predicted_output)\n",
    "            d_theta = calc_gradient(batch_samples_number,predicted_output,real_output_data_batch,training_data_batch)\n",
    "            theta_matrix,momentum = nesterov_acceleration_weight_update(theta_matrix,gamma,lr,momentum,training_data_batch,batch_samples_number,real_output_data_batch)\n",
    "            cost_history[batch+epoch_index] = cost\n",
    "            theta_history[batch+epoch_index] = theta_matrix\n",
    "            print(f\"Epoch:{epoch} of {epochs}, Batch: {batch}\\nCost: {cost}, Gradient: {d_theta}\")\n",
    "        print(f\"R2 Score: {r2_score(real_output,predict(training_data,theta_matrix))}\")\n",
    "        if epoch > 1 and (d_theta <= 0.00001 or sum(theta_history[batch+epoch_index-2]-theta_matrix)/len(theta_matrix) <= 0.00001 or cost_history[batch+epoch_index-2]-cost <= 0.00001):\n",
    "            break\n",
    "        epoch_index += (batches+1)\n",
    "    theta_history = prepare_theta_history(theta_history)\n",
    "    return {\"weights_matrix\":theta_matrix,\"predicted_output\":predicted_output,\"epochs_history\":cost_history,\"theta_history\":theta_history}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    raw_input_data = x\n",
    "    raw_output_data = y\n",
    "    prepared_input_data = prepare_input_data_for_training(raw_input_data)\n",
    "    model_results = solve_using_nesterov_accelerated_gradient_descent(prepared_input_data,raw_output_data,500,0.9,0.001)\n",
    "    fig = plt.figure(figsize=(10, 30), dpi=100)\n",
    "    print_graph(fig,raw_input_data,model_results[\"predicted_output\"],raw_output_data)\n",
    "    print_debug_graph(fig,model_results[\"epochs_history\"])\n",
    "    print_theta(fig,model_results[\"epochs_history\"],model_results[\"theta_history\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}